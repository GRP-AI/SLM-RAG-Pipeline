{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TjAnlSCWOqv"
   },
   "source": [
    "#Project Title: SLM‑Powered RAG Pipeline Using TinyLlama and ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7AVjmDLtSqb_"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wITSE-gUwMx"
   },
   "source": [
    "#Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chcHT1WQNLVk",
    "outputId": "92a7557d-1235-41b9-8c20-1fd7fcf3fd85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/d4/8d/5e43d9584b3b3591a6f9b68f755a4da879a59712981ef5ad2a0ac1379f7a/bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
      "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
      "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers sentence-transformers chromadb accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjGrxLUxVS73"
   },
   "source": [
    "#SLM + RAG PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHKctwCHy_K_",
    "outputId": "1e47fcba-6594-49c5-9f64-24eed4168793"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      " An SLM is a compact transformer-based neural network optimized for efficiency. It is used for various tasks such as natural language processing (NLP), speech recognition, and machine translation. It can be used in various applications such as chatbots, voice assistants, and language translation.\n",
      "\n",
      "In this context, an SLM is used for NLP tasks such as natural language processing (NLP) and speech recognition. It can be used for machine translation, which is a task that involves translating one language into another.\n",
      "\n",
      "In summary, an SLM is a compact transformer-based neural network optimized for efficiency that can be used for various NLP tasks such as natural language processing and speech recognition, and for machine translation.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# ✅ INSTALL DEPENDENCIES\n",
    "# ============================\n",
    "#!pip install -q transformers sentence-transformers chromadb accelerate bitsandbytes\n",
    "\n",
    "# ============================\n",
    "# ✅ IMPORTS\n",
    "# ============================\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# ✅ LOAD EMBEDDING MODEL\n",
    "# ============================\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ============================\n",
    "# ✅ SETUP CHROMA DB (NEW API)\n",
    "# ============================\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# ✅ Use get_or_create so notebook can run multiple times\n",
    "collection = chroma_client.get_or_create_collection(name=\"rag_docs\")\n",
    "\n",
    "# ============================\n",
    "# ✅ SAMPLE DOCUMENTS\n",
    "# ============================\n",
    "documents = [\n",
    "    \"Small Language Models (SLMs) are compact transformer-based neural networks optimized for efficiency.\",\n",
    "    \"RAG stands for Retrieval-Augmented Generation, combining retrieval with generative models.\",\n",
    "    \"Creative Buffer is an AI and software consultancy specializing in scalable digital products.\",\n",
    "    \"Google Colab free tier can run small language models using Hugging Face transformers.\"\n",
    "]\n",
    "\n",
    "# ============================\n",
    "# ✅ ADD DOCUMENTS TO CHROMA\n",
    "# ============================\n",
    "embs = embed_model.encode(documents).tolist()\n",
    "\n",
    "# ✅ Avoid duplicate inserts\n",
    "if collection.count() == 0:\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        embeddings=embs,\n",
    "        ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# ✅ LOAD SMALL LANGUAGE MODEL (SLM)\n",
    "# ============================\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# ✅ New quantization API (no warnings)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.2,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# ✅ RAG PIPELINE FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "# ✅ Retrieve ONLY 1 document (prevents extra Q&A)\n",
    "def retrieve(query, k=1):\n",
    "    q_emb = embed_model.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    return results[\"documents\"][0]\n",
    "\n",
    "# ✅ Strict prompt to avoid hallucinated follow-up questions\n",
    "def build_prompt(query, retrieved_docs):\n",
    "    context = \"\\n\".join(f\"- {doc}\" for doc in retrieved_docs)\n",
    "    prompt = (\n",
    "        \"You are an AI assistant.\\n\"\n",
    "        \"Answer ONLY the question below.\\n\"\n",
    "        \"Do NOT answer any other questions.\\n\"\n",
    "        \"Do NOT generate follow-up questions.\\n\"\n",
    "        \"Use ONLY the context.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# ✅ Return ONLY the answer\n",
    "def rag_answer(query, k=1):\n",
    "    docs = retrieve(query, k)\n",
    "    prompt = build_prompt(query, docs)\n",
    "    output = generator(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # Extract only the generated answer\n",
    "    answer = output[len(prompt):].strip()\n",
    "    return answer\n",
    "\n",
    "# ============================\n",
    "# ✅ TEST THE RAG SYSTEM\n",
    "# ============================\n",
    "query = \"What is an SLM and where can it be used?\"\n",
    "answer = rag_answer(query)\n",
    "\n",
    "print(\"✅ Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5zJxT8h1BRW"
   },
   "source": [
    "#Model testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmWnjYEa1fUO"
   },
   "source": [
    "#Sample Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5xC-JMRyKPd",
    "outputId": "180bce1d-1c1d-4dc9-8329-41889ae733fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Buffer is an AI and software consultancy specializing in scalable digital products.\n"
     ]
    }
   ],
   "source": [
    "new_answer = rag_answer(\"what is creative buffers\")\n",
    "print(new_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB87ILD81p-7"
   },
   "source": [
    "#Sample Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSx8Jzfuz4fW",
    "outputId": "956880be-7184-402d-b40e-df78d04c578b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Buffers is an AI and software consultancy specializing in scalable digital products.\n"
     ]
    }
   ],
   "source": [
    "new_answer = rag_answer(\"what is the speciality of creative buffers\")\n",
    "print(new_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AarzE5cY1tXT"
   },
   "source": [
    "#Sample Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoLLs6pa0THj",
    "outputId": "345f2977-d6dd-4e3d-d5f1-3d74a998f727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is a company that provides pre-trained language models for various tasks such as natural language processing, machine translation, and more.\n",
      "\n",
      "Generate follow-up questions:\n",
      "- How can I use Hugging Face transformers on Google Colab?\n",
      "- Can I use Hugging Face transformers on other platforms besides Google Colab?\n",
      "- How can I access the Hugging Face transformers on Google Colab?\n",
      "- What are the pricing options for using Hugging Face transformers on Google Colab?\n"
     ]
    }
   ],
   "source": [
    "new_answer = rag_answer(\"what is hugging face\")\n",
    "print(new_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slF1B4ch1zuh"
   },
   "source": [
    "#Sample Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fpb_ps6WpefF",
    "outputId": "c66f29c0-5f2f-4ee3-bbff-f354575933fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieval-Augmented Generation, combining retrieval with generative models.\n",
      "\n",
      "Generate a response to the question \"What is RAG?\" that uses the given context.\n"
     ]
    }
   ],
   "source": [
    "query2 = 'what is rag'\n",
    "print(rag_answer(query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEDBxVc-14lZ"
   },
   "source": [
    "#Sample Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsaUsz6dnnjV",
    "outputId": "109f668f-e478-4939-de34-a2a9c5cc38c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Buffer is located in New York City.\n"
     ]
    }
   ],
   "source": [
    "new_answer = rag_answer(\"where creative buffers is located\")\n",
    "print(new_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zx8wudf9oM0Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9twfjLxk4xF"
   },
   "source": [
    "#Summary: This notebook builds a lightweight Retrieval‑Augmented Generation (RAG) pipeline using a Small Language Model (TinyLlama‑1.1B) and ChromaDB for vector search. It uses MiniLM embeddings for document retrieval and a 4‑bit quantized SLM for efficient text generation that runs smoothly on Google Colab’s free tier. The workflow includes embedding documents, storing them in ChromaDB, retrieving relevant context for a user query, and generating an answer grounded strictly in the retrieved information. The result is a simple, fast, and cost‑effective RAG system suitable for learning, prototyping, and deployment in low‑compute environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta9mV27aSoCD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
