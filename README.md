ðŸ“˜ SLMâ€‘RAG: Lightweight Retrievalâ€‘Augmented Generation on Colab

A minimal, fast, and efficient Retrievalâ€‘Augmented Generation (RAG) pipeline built using Small Language Models (SLMs), MiniLM embeddings, and ChromaDB, fully optimized to run on Google Colabâ€™s free GPU tier.

This notebook shows how to combine a 4â€‘bitâ€‘quantized TinyLlama model with semantic retrieval to produce grounded, lowâ€‘latency answers using minimal compute.

âœ… Features:

Small Language Model (TinyLlamaâ€‘1.1B) in 4â€‘bit quantization

MiniLMâ€‘L6â€‘v2 embeddings for fast semantic search

ChromaDB vector store for retrieval

Strict prompt design to reduce hallucination

Retrieves only the most relevant document

Runs entirely on free Colab GPU

ðŸš€ How It Works: 

Embed documents using MiniLM

Store vectors in ChromaDB

Encode the query

Retrieve the top relevant document

Build a strict contextâ€‘only prompt

Generate the final answer using TinyLlama
